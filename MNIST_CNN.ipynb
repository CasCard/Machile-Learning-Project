{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMF8eqVidCgZwhhpzuX/exq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasCard/Machine-Learning-Project/blob/master/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI7qaytXjW4W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "65d5c225-ec9d-4154-cbdc-5b047137ddf0"
      },
      "source": [
        "from __future__ import print_function\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGpIr-IHrlp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist=tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcPXX644vr07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset=x_train.reshape([x_train.shape[0],x_train.shape[1],x_train.shape[2],1])\n",
        "test_dataset=x_test.reshape([x_test.shape[0],x_test.shape[1],x_test.shape[2],1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo5Xr1a_sPm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "288f544a-d3cc-4c8b-b419-b7f260ab692d"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(train_dataset.shape)\n",
        "print(test_dataset.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78_Z_58NsWSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def One_hot_encoder(labels,C):\n",
        "    C=tf.constant(C,dtype=tf.int32,name='C')\n",
        "    encoded=tf.one_hot(labels,C)\n",
        "    with tf.Session() as sess:\n",
        "        return sess.run(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIRqDq8Ou1Wv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cbd94266-c600-4d31-8066-7e18c7d7f1b1"
      },
      "source": [
        "Train_labels=One_hot_encoder(y_train,10)\n",
        "Test_labels=One_hot_encoder(y_test,10)\n",
        "print(Train_labels.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrFvcIAuu_rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
        "    X = tf.placeholder(shape=(None, n_H0, n_W0, n_C0),dtype=tf.float32)\n",
        "    Y = tf.placeholder(shape=(None, n_y),dtype=tf.float32)\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PELtNK0vGCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters():\n",
        "    W1 = tf.get_variable('W1',[4,4,1,8],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
        "    W2 = tf.get_variable('W2',[2,2,8,16],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"W2\": W2}\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQyiP_ItvMYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME')\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    P1 = tf.nn.max_pool(A1,padding='SAME',ksize=[1,8,8,1],strides=[1,8,8,1])\n",
        "    Z2 = tf.nn.conv2d(P1,W2,strides=[1,1,1,1],padding='SAME')\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    P2 = tf.nn.max_pool(A2,padding='SAME',ksize=[1,4,4,1],strides=[1,4,4,1])\n",
        "    P2 = tf.contrib.layers.flatten(P2)\n",
        "    Z3 = tf.contrib.layers.fully_connected(P2,10,activation_fn=None)\n",
        "    return Z3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106XhSuDvQyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(Z3, Y):\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x8iRMD2vVej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c164c4bd-51d1-4802-8447-203fd64bcd3d"
      },
      "source": [
        "import time as t\n",
        "X, Y = create_placeholders(28,28,1,10)\n",
        "parameters = initialize_parameters()\n",
        "Z3 = forward_propagation(X, parameters)\n",
        "cost = compute_cost(Z3, Y)\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "init = tf.global_variables_initializer()\n",
        "predict_op = tf.argmax(Z3, 1)\n",
        "correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))        \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(\"Training Started\")\n",
        "    A=t.time()\n",
        "    for i in range(500):\n",
        "        _ , temp_cost,acc = sess.run([optimizer,cost,accuracy],{X:train_dataset,Y:Train_labels})\n",
        "        print(i,\" : \",temp_cost,\" : \",acc)\n",
        "    print('Training Over time taken ',t.time()-A,'sec')\n",
        "    A=t.time()\n",
        "    print('Testing Accuracy',sess.run(accuracy,{X:test_dataset,Y:Test_labels}))\n",
        "    print('Testing Time',t.time()-A)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-10-d63c9aaddfe2>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Training Started\n",
            "0  :  141.3339  :  0.11906666\n",
            "1  :  129.22856  :  0.13741666\n",
            "2  :  119.209366  :  0.15423334\n",
            "3  :  111.204216  :  0.16031666\n",
            "4  :  104.57083  :  0.1599\n",
            "5  :  98.6723  :  0.15908334\n",
            "6  :  92.95895  :  0.16078334\n",
            "7  :  87.167725  :  0.16411667\n",
            "8  :  81.33274  :  0.16726667\n",
            "9  :  75.65827  :  0.16888334\n",
            "10  :  70.49505  :  0.16665\n",
            "11  :  66.25515  :  0.15588333\n",
            "12  :  63.171947  :  0.13751666\n",
            "13  :  61.102818  :  0.1217\n",
            "14  :  59.60055  :  0.112616666\n",
            "15  :  58.222572  :  0.1114\n",
            "16  :  56.710026  :  0.11246666\n",
            "17  :  54.98716  :  0.116466664\n",
            "18  :  53.08752  :  0.120916665\n",
            "19  :  51.093224  :  0.12531666\n",
            "20  :  49.08892  :  0.12841667\n",
            "21  :  47.131332  :  0.13218333\n",
            "22  :  45.243534  :  0.1339\n",
            "23  :  43.41777  :  0.1355\n",
            "24  :  41.6313  :  0.1364\n",
            "25  :  39.86099  :  0.1362\n",
            "26  :  38.09644  :  0.13595\n",
            "27  :  36.34544  :  0.1345\n",
            "28  :  34.62607  :  0.13251667\n",
            "29  :  32.960857  :  0.13141666\n",
            "30  :  31.367807  :  0.13043334\n",
            "31  :  29.85915  :  0.12898333\n",
            "32  :  28.440716  :  0.12718333\n",
            "33  :  27.11566  :  0.12623334\n",
            "34  :  25.89343  :  0.12548333\n",
            "35  :  24.806168  :  0.12478333\n",
            "36  :  23.898462  :  0.123966664\n",
            "37  :  23.191387  :  0.12531666\n",
            "38  :  22.641125  :  0.12553333\n",
            "39  :  22.146118  :  0.12573333\n",
            "40  :  21.599567  :  0.12638333\n",
            "41  :  20.929445  :  0.12713334\n",
            "42  :  20.12513  :  0.12821667\n",
            "43  :  19.236145  :  0.13121666\n",
            "44  :  18.358236  :  0.13483334\n",
            "45  :  17.59616  :  0.13785\n",
            "46  :  17.026659  :  0.13936667\n",
            "47  :  16.655146  :  0.13936667\n",
            "48  :  16.40855  :  0.13893333\n",
            "49  :  16.186165  :  0.13765\n",
            "50  :  15.917457  :  0.13748333\n",
            "51  :  15.585823  :  0.13803333\n",
            "52  :  15.213194  :  0.13946667\n",
            "53  :  14.82923  :  0.14133333\n",
            "54  :  14.449069  :  0.14355\n",
            "55  :  14.071757  :  0.14588334\n",
            "56  :  13.689521  :  0.14836666\n",
            "57  :  13.300641  :  0.15111667\n",
            "58  :  12.914284  :  0.15478334\n",
            "59  :  12.549648  :  0.15921667\n",
            "60  :  12.229828  :  0.164\n",
            "61  :  11.973613  :  0.16843334\n",
            "62  :  11.786966  :  0.17211667\n",
            "63  :  11.657518  :  0.1755\n",
            "64  :  11.561262  :  0.17921667\n",
            "65  :  11.471091  :  0.18173334\n",
            "66  :  11.368777  :  0.18236667\n",
            "67  :  11.2493105  :  0.183\n",
            "68  :  11.119489  :  0.18418333\n",
            "69  :  10.990847  :  0.18438333\n",
            "70  :  10.87143  :  0.18456666\n",
            "71  :  10.761213  :  0.18496667\n",
            "72  :  10.652579  :  0.1858\n",
            "73  :  10.536035  :  0.18738334\n",
            "74  :  10.405655  :  0.19015\n",
            "75  :  10.262892  :  0.1938\n",
            "76  :  10.1157465  :  0.19818333\n",
            "77  :  9.974981  :  0.20225\n",
            "78  :  9.848897  :  0.20541666\n",
            "79  :  9.7396965  :  0.20921667\n",
            "80  :  9.643817  :  0.21213333\n",
            "81  :  9.554477  :  0.21478334\n",
            "82  :  9.465435  :  0.21668333\n",
            "83  :  9.373222  :  0.21838333\n",
            "84  :  9.278093  :  0.21998334\n",
            "85  :  9.182871  :  0.22108333\n",
            "86  :  9.09101  :  0.2219\n",
            "87  :  9.004426  :  0.22331667\n",
            "88  :  8.923087  :  0.22426666\n",
            "89  :  8.845298  :  0.22525\n",
            "90  :  8.769033  :  0.22716667\n",
            "91  :  8.692873  :  0.22896667\n",
            "92  :  8.616708  :  0.23098333\n",
            "93  :  8.541471  :  0.23311667\n",
            "94  :  8.468218  :  0.23546667\n",
            "95  :  8.397495  :  0.2374\n",
            "96  :  8.328894  :  0.23956667\n",
            "97  :  8.261482  :  0.24193333\n",
            "98  :  8.194296  :  0.24368334\n",
            "99  :  8.126878  :  0.245\n",
            "100  :  8.059604  :  0.24651666\n",
            "101  :  7.993303  :  0.24803333\n",
            "102  :  7.9287977  :  0.24901667\n",
            "103  :  7.866382  :  0.25013334\n",
            "104  :  7.805694  :  0.25158334\n",
            "105  :  7.746158  :  0.25285\n",
            "106  :  7.687249  :  0.2547\n",
            "107  :  7.6287556  :  0.25618333\n",
            "108  :  7.570873  :  0.25755\n",
            "109  :  7.513948  :  0.25883332\n",
            "110  :  7.4582996  :  0.26078334\n",
            "111  :  7.403988  :  0.26258335\n",
            "112  :  7.3508253  :  0.26385\n",
            "113  :  7.2985134  :  0.26505\n",
            "114  :  7.2467637  :  0.26623333\n",
            "115  :  7.1954637  :  0.26746666\n",
            "116  :  7.1446834  :  0.26881668\n",
            "117  :  7.0946136  :  0.26998332\n",
            "118  :  7.045432  :  0.271\n",
            "119  :  6.9971375  :  0.27221668\n",
            "120  :  6.9496436  :  0.27396667\n",
            "121  :  6.9028606  :  0.2753\n",
            "122  :  6.8566875  :  0.27716666\n",
            "123  :  6.811022  :  0.2785\n",
            "124  :  6.7659116  :  0.28005\n",
            "125  :  6.721424  :  0.28171667\n",
            "126  :  6.677499  :  0.28348333\n",
            "127  :  6.634158  :  0.2852\n",
            "128  :  6.5913677  :  0.28673333\n",
            "129  :  6.5490656  :  0.28818333\n",
            "130  :  6.5072794  :  0.28995\n",
            "131  :  6.4659877  :  0.291\n",
            "132  :  6.4252825  :  0.29236665\n",
            "133  :  6.3851876  :  0.29366666\n",
            "134  :  6.3456388  :  0.29475\n",
            "135  :  6.3065996  :  0.29606667\n",
            "136  :  6.2680616  :  0.29751667\n",
            "137  :  6.2299714  :  0.2984\n",
            "138  :  6.19233  :  0.2999\n",
            "139  :  6.1551156  :  0.30128333\n",
            "140  :  6.118335  :  0.30241665\n",
            "141  :  6.081923  :  0.30368334\n",
            "142  :  6.0458546  :  0.30506667\n",
            "143  :  6.0101643  :  0.30636665\n",
            "144  :  5.9748387  :  0.30721667\n",
            "145  :  5.939878  :  0.30816665\n",
            "146  :  5.905322  :  0.30911666\n",
            "147  :  5.8711376  :  0.31026667\n",
            "148  :  5.837302  :  0.31156668\n",
            "149  :  5.8037825  :  0.31275\n",
            "150  :  5.770589  :  0.31398332\n",
            "151  :  5.7377086  :  0.31505\n",
            "152  :  5.705122  :  0.3158\n",
            "153  :  5.672848  :  0.3173\n",
            "154  :  5.6409307  :  0.31836668\n",
            "155  :  5.609356  :  0.3195\n",
            "156  :  5.5780854  :  0.3204\n",
            "157  :  5.5471  :  0.32138333\n",
            "158  :  5.5164084  :  0.32245\n",
            "159  :  5.4859886  :  0.32378334\n",
            "160  :  5.455855  :  0.32516667\n",
            "161  :  5.425979  :  0.32613334\n",
            "162  :  5.396368  :  0.3273\n",
            "163  :  5.3670316  :  0.32783332\n",
            "164  :  5.3379354  :  0.32903334\n",
            "165  :  5.309084  :  0.32995\n",
            "166  :  5.280475  :  0.3309\n",
            "167  :  5.2520895  :  0.33223334\n",
            "168  :  5.223973  :  0.33316666\n",
            "169  :  5.196074  :  0.3339\n",
            "170  :  5.1684113  :  0.33531666\n",
            "171  :  5.1409388  :  0.33638334\n",
            "172  :  5.113646  :  0.33695\n",
            "173  :  5.0866017  :  0.3381\n",
            "174  :  5.0598264  :  0.33928335\n",
            "175  :  5.033298  :  0.34023333\n",
            "176  :  5.0069814  :  0.34158334\n",
            "177  :  4.9808254  :  0.34278333\n",
            "178  :  4.9548416  :  0.34358335\n",
            "179  :  4.929043  :  0.34463334\n",
            "180  :  4.903451  :  0.34528333\n",
            "181  :  4.8780127  :  0.34645\n",
            "182  :  4.8527603  :  0.34728333\n",
            "183  :  4.8276806  :  0.34818333\n",
            "184  :  4.802838  :  0.34921667\n",
            "185  :  4.7782116  :  0.34998333\n",
            "186  :  4.753756  :  0.35068333\n",
            "187  :  4.7294226  :  0.35158333\n",
            "188  :  4.705258  :  0.35248333\n",
            "189  :  4.6813  :  0.35361665\n",
            "190  :  4.65756  :  0.35485\n",
            "191  :  4.634061  :  0.35565\n",
            "192  :  4.610764  :  0.35641667\n",
            "193  :  4.5876384  :  0.35735\n",
            "194  :  4.564723  :  0.35836667\n",
            "195  :  4.541997  :  0.35963333\n",
            "196  :  4.519463  :  0.36065\n",
            "197  :  4.4970946  :  0.36153334\n",
            "198  :  4.474898  :  0.36241665\n",
            "199  :  4.452884  :  0.36323333\n",
            "200  :  4.4310646  :  0.36393332\n",
            "201  :  4.4094405  :  0.36458334\n",
            "202  :  4.3879848  :  0.36555\n",
            "203  :  4.366707  :  0.36626667\n",
            "204  :  4.345603  :  0.36715\n",
            "205  :  4.324664  :  0.36815\n",
            "206  :  4.303912  :  0.36906666\n",
            "207  :  4.2833347  :  0.37021667\n",
            "208  :  4.2629395  :  0.37096667\n",
            "209  :  4.2427177  :  0.37233335\n",
            "210  :  4.2226515  :  0.37298334\n",
            "211  :  4.2027454  :  0.37398332\n",
            "212  :  4.1830606  :  0.37495\n",
            "213  :  4.163532  :  0.37568334\n",
            "214  :  4.144146  :  0.37688333\n",
            "215  :  4.1249256  :  0.3777\n",
            "216  :  4.1058736  :  0.37868333\n",
            "217  :  4.0870028  :  0.37961668\n",
            "218  :  4.0682883  :  0.38038334\n",
            "219  :  4.049723  :  0.38156667\n",
            "220  :  4.031322  :  0.38271666\n",
            "221  :  4.01307  :  0.38351667\n",
            "222  :  3.9949744  :  0.3844\n",
            "223  :  3.9770272  :  0.38538334\n",
            "224  :  3.9592488  :  0.38636667\n",
            "225  :  3.9416282  :  0.38686666\n",
            "226  :  3.9241529  :  0.38765\n",
            "227  :  3.9068062  :  0.38865\n",
            "228  :  3.889627  :  0.38965\n",
            "229  :  3.8726096  :  0.39028335\n",
            "230  :  3.8557348  :  0.39131665\n",
            "231  :  3.8389926  :  0.39228332\n",
            "232  :  3.822379  :  0.39365\n",
            "233  :  3.8058605  :  0.39453334\n",
            "234  :  3.7894542  :  0.3955\n",
            "235  :  3.7731407  :  0.3965\n",
            "236  :  3.7569308  :  0.39716667\n",
            "237  :  3.740834  :  0.39788333\n",
            "238  :  3.7248664  :  0.39856666\n",
            "239  :  3.7090132  :  0.39945\n",
            "240  :  3.6932526  :  0.40031666\n",
            "241  :  3.6775901  :  0.4014\n",
            "242  :  3.6620367  :  0.40208334\n",
            "243  :  3.6465797  :  0.40271667\n",
            "244  :  3.6312315  :  0.40341666\n",
            "245  :  3.6159806  :  0.40395\n",
            "246  :  3.6008065  :  0.40481666\n",
            "247  :  3.5857031  :  0.4059\n",
            "248  :  3.5706954  :  0.40673333\n",
            "249  :  3.5557919  :  0.40746668\n",
            "250  :  3.541007  :  0.40856665\n",
            "251  :  3.5263245  :  0.40918332\n",
            "252  :  3.511742  :  0.40998334\n",
            "253  :  3.497242  :  0.4107\n",
            "254  :  3.4828265  :  0.41218334\n",
            "255  :  3.4685218  :  0.41318333\n",
            "256  :  3.454313  :  0.41396666\n",
            "257  :  3.4401822  :  0.41465\n",
            "258  :  3.4261277  :  0.41573334\n",
            "259  :  3.412144  :  0.41643333\n",
            "260  :  3.3982294  :  0.41753334\n",
            "261  :  3.3844054  :  0.4186\n",
            "262  :  3.3706615  :  0.41898334\n",
            "263  :  3.356973  :  0.41968334\n",
            "264  :  3.3433743  :  0.42021668\n",
            "265  :  3.3298442  :  0.42073333\n",
            "266  :  3.3163714  :  0.42158332\n",
            "267  :  3.3029633  :  0.42233333\n",
            "268  :  3.289616  :  0.42321667\n",
            "269  :  3.276336  :  0.424\n",
            "270  :  3.2631307  :  0.42511666\n",
            "271  :  3.2500274  :  0.42581666\n",
            "272  :  3.236988  :  0.42691666\n",
            "273  :  3.2240338  :  0.42758334\n",
            "274  :  3.211153  :  0.42836666\n",
            "275  :  3.1983464  :  0.42928332\n",
            "276  :  3.1855927  :  0.43001667\n",
            "277  :  3.172922  :  0.4311\n",
            "278  :  3.1603346  :  0.43195\n",
            "279  :  3.1478229  :  0.43273333\n",
            "280  :  3.1354113  :  0.43376666\n",
            "281  :  3.1230803  :  0.4345\n",
            "282  :  3.110812  :  0.43513334\n",
            "283  :  3.0986311  :  0.43638334\n",
            "284  :  3.0865312  :  0.43741667\n",
            "285  :  3.0745118  :  0.43805\n",
            "286  :  3.0625541  :  0.43875\n",
            "287  :  3.050651  :  0.43948334\n",
            "288  :  3.0388215  :  0.4404\n",
            "289  :  3.0270584  :  0.44106665\n",
            "290  :  3.0153701  :  0.44205\n",
            "291  :  3.0037668  :  0.44296667\n",
            "292  :  2.992241  :  0.4438\n",
            "293  :  2.9807966  :  0.4449\n",
            "294  :  2.9694307  :  0.44566667\n",
            "295  :  2.9581313  :  0.44631666\n",
            "296  :  2.9468927  :  0.4469\n",
            "297  :  2.9357212  :  0.44796667\n",
            "298  :  2.924624  :  0.44906667\n",
            "299  :  2.9135935  :  0.44986665\n",
            "300  :  2.902632  :  0.45075\n",
            "301  :  2.891728  :  0.4517\n",
            "302  :  2.8808954  :  0.45233333\n",
            "303  :  2.8701289  :  0.45293334\n",
            "304  :  2.8594246  :  0.45401666\n",
            "305  :  2.8487854  :  0.45511666\n",
            "306  :  2.838196  :  0.45593333\n",
            "307  :  2.827664  :  0.45701668\n",
            "308  :  2.817208  :  0.45811668\n",
            "309  :  2.8068204  :  0.4592\n",
            "310  :  2.796508  :  0.46005\n",
            "311  :  2.7862709  :  0.4609\n",
            "312  :  2.7760935  :  0.46166667\n",
            "313  :  2.7659786  :  0.46255\n",
            "314  :  2.755924  :  0.46368334\n",
            "315  :  2.7459447  :  0.46443334\n",
            "316  :  2.73604  :  0.46543333\n",
            "317  :  2.7261953  :  0.46605\n",
            "318  :  2.716422  :  0.46666667\n",
            "319  :  2.7067099  :  0.4679\n",
            "320  :  2.697052  :  0.4688\n",
            "321  :  2.687458  :  0.46981665\n",
            "322  :  2.6779404  :  0.47065\n",
            "323  :  2.668492  :  0.47173333\n",
            "324  :  2.6591084  :  0.47231665\n",
            "325  :  2.6497905  :  0.47308335\n",
            "326  :  2.6405456  :  0.47403333\n",
            "327  :  2.6313624  :  0.47468334\n",
            "328  :  2.6222372  :  0.47518334\n",
            "329  :  2.6131625  :  0.476\n",
            "330  :  2.6041355  :  0.47715\n",
            "331  :  2.59516  :  0.47806665\n",
            "332  :  2.5862308  :  0.4789\n",
            "333  :  2.5773394  :  0.47965\n",
            "334  :  2.5684984  :  0.48045\n",
            "335  :  2.5597215  :  0.48105\n",
            "336  :  2.5509982  :  0.48195\n",
            "337  :  2.542329  :  0.4825\n",
            "338  :  2.5337124  :  0.48343334\n",
            "339  :  2.525149  :  0.48418334\n",
            "340  :  2.5166388  :  0.48506665\n",
            "341  :  2.508188  :  0.48608333\n",
            "342  :  2.499786  :  0.48695\n",
            "343  :  2.491435  :  0.48786667\n",
            "344  :  2.483136  :  0.48853335\n",
            "345  :  2.4748862  :  0.48906666\n",
            "346  :  2.466687  :  0.48981667\n",
            "347  :  2.458534  :  0.49053332\n",
            "348  :  2.4504333  :  0.49156666\n",
            "349  :  2.4423904  :  0.49228334\n",
            "350  :  2.4343984  :  0.49288332\n",
            "351  :  2.426459  :  0.49371666\n",
            "352  :  2.418571  :  0.4946\n",
            "353  :  2.4107318  :  0.49543333\n",
            "354  :  2.402937  :  0.49615\n",
            "355  :  2.395186  :  0.497\n",
            "356  :  2.3874812  :  0.49785\n",
            "357  :  2.3798258  :  0.4987\n",
            "358  :  2.3722181  :  0.49961665\n",
            "359  :  2.3646533  :  0.5003167\n",
            "360  :  2.3571358  :  0.50116664\n",
            "361  :  2.3496656  :  0.50196666\n",
            "362  :  2.3422365  :  0.5025667\n",
            "363  :  2.3348503  :  0.50333333\n",
            "364  :  2.3275046  :  0.5040333\n",
            "365  :  2.3202  :  0.5046333\n",
            "366  :  2.312934  :  0.50553334\n",
            "367  :  2.3057034  :  0.50638336\n",
            "368  :  2.298517  :  0.50736666\n",
            "369  :  2.2913752  :  0.50806665\n",
            "370  :  2.2842734  :  0.5086667\n",
            "371  :  2.2772114  :  0.50975\n",
            "372  :  2.270189  :  0.51035\n",
            "373  :  2.2632058  :  0.51098335\n",
            "374  :  2.2562592  :  0.51185\n",
            "375  :  2.249357  :  0.5126167\n",
            "376  :  2.242498  :  0.5133833\n",
            "377  :  2.2356822  :  0.51413333\n",
            "378  :  2.2289119  :  0.5150333\n",
            "379  :  2.2221844  :  0.5157667\n",
            "380  :  2.215491  :  0.51663333\n",
            "381  :  2.208837  :  0.51736665\n",
            "382  :  2.20222  :  0.51823336\n",
            "383  :  2.1956425  :  0.51923335\n",
            "384  :  2.189106  :  0.51963335\n",
            "385  :  2.182614  :  0.5202\n",
            "386  :  2.1761625  :  0.52115\n",
            "387  :  2.169749  :  0.5218\n",
            "388  :  2.1633725  :  0.52248335\n",
            "389  :  2.1570365  :  0.5233333\n",
            "390  :  2.1507382  :  0.52431667\n",
            "391  :  2.1444721  :  0.52503335\n",
            "392  :  2.138241  :  0.5259167\n",
            "393  :  2.132053  :  0.5265\n",
            "394  :  2.125895  :  0.5272833\n",
            "395  :  2.1197705  :  0.5280167\n",
            "396  :  2.1136785  :  0.52875\n",
            "397  :  2.107617  :  0.52993333\n",
            "398  :  2.1015887  :  0.53076667\n",
            "399  :  2.0955904  :  0.5313333\n",
            "400  :  2.0896292  :  0.53195\n",
            "401  :  2.0836997  :  0.53253335\n",
            "402  :  2.0777977  :  0.53323334\n",
            "403  :  2.0719264  :  0.53388333\n",
            "404  :  2.0660892  :  0.5344667\n",
            "405  :  2.0602844  :  0.53505\n",
            "406  :  2.0545127  :  0.5358667\n",
            "407  :  2.0487792  :  0.5366167\n",
            "408  :  2.0430737  :  0.53718334\n",
            "409  :  2.0373983  :  0.53786665\n",
            "410  :  2.031764  :  0.5386\n",
            "411  :  2.0261638  :  0.5392333\n",
            "412  :  2.0205884  :  0.53995\n",
            "413  :  2.0150445  :  0.5405333\n",
            "414  :  2.009531  :  0.5415\n",
            "415  :  2.0040436  :  0.5423167\n",
            "416  :  1.998582  :  0.54298335\n",
            "417  :  1.9931533  :  0.5437667\n",
            "418  :  1.9877546  :  0.5446\n",
            "419  :  1.9823872  :  0.5454\n",
            "420  :  1.9770505  :  0.54606664\n",
            "421  :  1.9717476  :  0.54686666\n",
            "422  :  1.9664699  :  0.5473667\n",
            "423  :  1.9612149  :  0.5479\n",
            "424  :  1.955985  :  0.54863334\n",
            "425  :  1.9507766  :  0.54903334\n",
            "426  :  1.9455947  :  0.5497\n",
            "427  :  1.9404359  :  0.5503833\n",
            "428  :  1.9353075  :  0.5513167\n",
            "429  :  1.930206  :  0.55175\n",
            "430  :  1.9251322  :  0.55245\n",
            "431  :  1.9200857  :  0.5528833\n",
            "432  :  1.9150616  :  0.55338335\n",
            "433  :  1.9100643  :  0.5541833\n",
            "434  :  1.9050894  :  0.5549667\n",
            "435  :  1.9001355  :  0.55541664\n",
            "436  :  1.8952044  :  0.55618334\n",
            "437  :  1.8902974  :  0.5567667\n",
            "438  :  1.8854096  :  0.5576\n",
            "439  :  1.8805412  :  0.55826664\n",
            "440  :  1.8756899  :  0.5588667\n",
            "441  :  1.8708571  :  0.5595667\n",
            "442  :  1.8660456  :  0.56025\n",
            "443  :  1.8612545  :  0.56116664\n",
            "444  :  1.8564837  :  0.56186664\n",
            "445  :  1.8517324  :  0.5623\n",
            "446  :  1.8469985  :  0.56306666\n",
            "447  :  1.8422852  :  0.5635833\n",
            "448  :  1.8375932  :  0.5641\n",
            "449  :  1.8329245  :  0.5646167\n",
            "450  :  1.8282758  :  0.56513333\n",
            "451  :  1.8236476  :  0.56588334\n",
            "452  :  1.8190422  :  0.56656665\n",
            "453  :  1.8144503  :  0.5671333\n",
            "454  :  1.8098781  :  0.5678167\n",
            "455  :  1.8053334  :  0.56845\n",
            "456  :  1.8008118  :  0.5693833\n",
            "457  :  1.7963097  :  0.56975\n",
            "458  :  1.7918327  :  0.5705\n",
            "459  :  1.7873737  :  0.5711167\n",
            "460  :  1.7829396  :  0.5715333\n",
            "461  :  1.7785274  :  0.57203335\n",
            "462  :  1.7741349  :  0.57275\n",
            "463  :  1.7697597  :  0.57345\n",
            "464  :  1.7654052  :  0.5739667\n",
            "465  :  1.7610768  :  0.57453334\n",
            "466  :  1.7567713  :  0.575\n",
            "467  :  1.7524837  :  0.57568336\n",
            "468  :  1.7482111  :  0.57635\n",
            "469  :  1.7439569  :  0.5769167\n",
            "470  :  1.7397232  :  0.5776167\n",
            "471  :  1.7355102  :  0.57808334\n",
            "472  :  1.7313138  :  0.57878333\n",
            "473  :  1.7271372  :  0.5792\n",
            "474  :  1.7229794  :  0.57985\n",
            "475  :  1.7188437  :  0.58028334\n",
            "476  :  1.7147274  :  0.58095\n",
            "477  :  1.7106293  :  0.58168334\n",
            "478  :  1.7065471  :  0.58241665\n",
            "479  :  1.7024791  :  0.58288336\n",
            "480  :  1.698431  :  0.58341664\n",
            "481  :  1.694404  :  0.5840333\n",
            "482  :  1.6903982  :  0.58456665\n",
            "483  :  1.6864091  :  0.58503336\n",
            "484  :  1.6824378  :  0.5858167\n",
            "485  :  1.6784834  :  0.58638334\n",
            "486  :  1.6745498  :  0.587\n",
            "487  :  1.6706417  :  0.5876833\n",
            "488  :  1.6667526  :  0.58845\n",
            "489  :  1.6628802  :  0.589\n",
            "490  :  1.6590245  :  0.58965\n",
            "491  :  1.6551841  :  0.59033334\n",
            "492  :  1.6513622  :  0.5911\n",
            "493  :  1.6475623  :  0.5915833\n",
            "494  :  1.6437799  :  0.59223336\n",
            "495  :  1.6400107  :  0.5930167\n",
            "496  :  1.6362557  :  0.5938\n",
            "497  :  1.6325153  :  0.59425\n",
            "498  :  1.6287882  :  0.59466666\n",
            "499  :  1.6250768  :  0.59526664\n",
            "Training Over time taken  92.93667817115784 sec\n",
            "Testing Accuracy 0.6039\n",
            "Testing Time 0.07980036735534668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTErxZ2gx4b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}